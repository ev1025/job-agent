import asyncio
import os
import json
from datetime import datetime, timedelta

from dotenv import load_dotenv
from google.cloud import storage
import vertexai
from vertexai import rag

from crawler.scrapers.saramin import crawl_saramin # ê¸°ì¡´ í¬ë¡¤ëŸ¬ ëª¨ë“ˆ

# --- 1. ì „ì²´ ì„¤ì • ---
load_dotenv()

# ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ì´ë¦„ ìƒì„±
today_date_str = datetime.now().strftime('%Y-%m-%d')

# GCP ë° Vertex AI ì„¤ì •
PROJECT_ID = "job-agent-471006"
LOCATION = "us-east4"
CORPUS_DISPLAY_NAME = f"job_corpus_{today_date_str}"

# GCS ì„¤ì •
GCS_BUCKET_NAME = "job-agent-raw-json"
GCS_DESTINATION_FOLDER = "rag-source-data"
GCS_FILENAME_PREFIX = f"job_{today_date_str}"
GCS_URI_FOR_RAG = f"gs://{GCS_BUCKET_NAME}/{GCS_DESTINATION_FOLDER}/"

# í¬ë¡¤ë§ ë° ë°°ì¹˜ ì„¤ì •
BATCH_SIZE = 500
TOTAL_PAGE_LIMIT = 1
SEARCH_KEYWORDS = ['ì²­ì†Œ']
# <<-- ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì— ë‹¤ì‹œ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.
SEARCH_START_DATE_STR = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')


# --- 2. GCS ì—…ë¡œë“œ í—¬í¼ í•¨ìˆ˜ ---

def upload_batch_to_gcs(jobs_batch: list, file_index: int, bucket):
    """ì±„ìš© ê³µê³  ë°°ì¹˜(list)ë¥¼ GCSì— part íŒŒì¼ë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
    if not jobs_batch:
        return

    gcs_blob_name = f"{GCS_DESTINATION_FOLDER}/{GCS_FILENAME_PREFIX}_part_{file_index}.jsonl"
    blob = bucket.blob(gcs_blob_name)

    jsonl_records = []
    for job in jobs_batch:
        content = job.get('ì œëª©', '') + "\n\n" + job.get('ìƒì„¸ë‚´ìš©', '')
        metadata = {k: v for k, v in job.items() if k not in ['rec_idx', 'ì œëª©', 'ìƒì„¸ë‚´ìš©']}
        json_record = {
            "id": str(job.get('rec_idx')),
            "structData": metadata,
            "content": content
        }
        jsonl_records.append(json.dumps(json_record, ensure_ascii=False))

    final_jsonl_content = "\n".join(jsonl_records)

    try:
        blob.upload_from_string(final_jsonl_content, content_type="application/jsonl")
        print(f"âœ… {len(jobs_batch)}ê°œ ê³µê³ ë¥¼ {gcs_blob_name} íŒŒì¼ë¡œ GCSì— ì—…ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"ğŸš¨ GCS ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (íŒŒì¼: {gcs_blob_name}): {e}")

# --- 3. RAG Engine ë¡œë“œ í—¬í¼ í•¨ìˆ˜ ---

def ingest_gcs_files_to_rag(gcs_uri: str):
    """ì§€ì •ëœ GCS ê²½ë¡œì˜ ëª¨ë“  íŒŒì¼ì„ RAG Engineìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    print("\n--- 2ë‹¨ê³„: RAG Engineìœ¼ë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹œì‘ ---")
    
    vertexai.init(project=PROJECT_ID, location=LOCATION)
    
    corpora = rag.list_corpora()
    corpus = next((c for c in corpora if c.display_name == CORPUS_DISPLAY_NAME), None)
    
    if not corpus:
        print(f"'{CORPUS_DISPLAY_NAME}' ì½”í¼ìŠ¤ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        corpus = rag.create_corpus(display_name=CORPUS_DISPLAY_NAME)
    else:
        print(f"ê¸°ì¡´ ì½”í¼ìŠ¤ '{CORPUS_DISPLAY_NAME}'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    print(f"'{gcs_uri}' ê²½ë¡œì˜ íŒŒì¼ë“¤ì„ RAG Engineìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    try:
        rag.import_files(
            corpus.name,
            [gcs_uri],
        )
        print("âœ… Storage -> VectorDB ì„í¬íŠ¸ ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("   (ì‹¤ì œ ì¸ë±ì‹± ì™„ë£Œê¹Œì§€ëŠ” ë°ì´í„° ì–‘ì— ë”°ë¼ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    except Exception as e:
        print(f"â— RAG Engine ì„í¬íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# --- 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---

async def main():
    """í¬ë¡¤ë§, GCS ì—…ë¡œë“œ, RAG Engine ì„í¬íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    start_time = datetime.now()

    # 1ë‹¨ê³„: í¬ë¡¤ë§ ë° GCS ë°°ì¹˜ ì—…ë¡œë“œ
    print(f"--- 1ë‹¨ê³„: í¬ë¡¤ë§ ë° GCS ë°°ì¹˜ ì—…ë¡œë“œ ì‹œì‘ (íŒŒì¼ëª… ì ‘ë‘ì‚¬: {GCS_FILENAME_PREFIX}) ---")
    storage_client = storage.Client(project=PROJECT_ID)
    bucket = storage_client.bucket(GCS_BUCKET_NAME)
    
    all_jobs_buffer = []
    total_jobs_uploaded = 0
    file_index = 1
    
    # ì—¬ê¸°ì„œ ì‚¬ìš©í•  ì‹œì‘ ë‚ ì§œ ê°ì²´ë¥¼ ë¯¸ë¦¬ ìƒì„±í•©ë‹ˆë‹¤.
    start_date_obj = datetime.strptime(SEARCH_START_DATE_STR, "%Y-%m-%d")

    for keyword in SEARCH_KEYWORDS:
        print(f"\n> í‚¤ì›Œë“œ: '{keyword}' í¬ë¡¤ë§ ì¤‘...")
        async for jobs_from_page in crawl_saramin(start_date_obj, set(), TOTAL_PAGE_LIMIT, keyword):
            all_jobs_buffer.extend(jobs_from_page)
            
            if len(all_jobs_buffer) >= BATCH_SIZE:
                batch_to_upload = all_jobs_buffer[:BATCH_SIZE]
                all_jobs_buffer = all_jobs_buffer[BATCH_SIZE:]

                upload_batch_to_gcs(batch_to_upload, file_index, bucket)
                
                total_jobs_uploaded += len(batch_to_upload)
                file_index += 1

    if all_jobs_buffer:
        upload_batch_to_gcs(all_jobs_buffer, file_index, bucket)
        total_jobs_uploaded += len(all_jobs_buffer)
    
    print(f"\n--- 1ë‹¨ê³„ ì™„ë£Œ: ì´ {total_jobs_uploaded}ê°œ ê³µê³ ë¥¼ GCSì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. ---")

    if total_jobs_uploaded > 0:
        await asyncio.to_thread(ingest_gcs_files_to_rag, GCS_URI_FOR_RAG)
    else:
        print("\n--- ì—…ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ì–´ RAG Engine ì„í¬íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤. ---")

    print(f"\n--- ëª¨ë“  ì‘ì—… ì™„ë£Œ ---")
    print(f"ì´ ì†Œìš” ì‹œê°„: {datetime.now() - start_time}")

if __name__ == "__main__":
    asyncio.run(main())