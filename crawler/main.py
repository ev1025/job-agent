import asyncio
import pandas as pd
import aiomysql
import os
from datetime import datetime, timedelta
from dotenv import load_dotenv

from crawler.scrapers.saramin import crawl_saramin

# --- CONFIGURATION ---
load_dotenv()

SEARCH_START_DATE_STR = (datetime.now() - timedelta(weeks=3)).strftime('%Y-%m-%d')
BATCH_SIZE = 500  # DB ì‚½ì… ë°°ì¹˜ ë‹¨ìœ„
TOTAL_PAGE_LIMIT = 100 # ì „ì²´ í˜ì´ì§€ ì œí•œ
SEARCH_KEYWORDS = ['LLM', 'ë°ì´í„° ë¶„ì„', 'AI','rag', 'agent'] # ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸

# --- DATABASE FUNCTIONS ---
async def save_final_jobs(pool, df: pd.DataFrame):
    """DataFrameì„ job_raw í…Œì´ë¸”ì— ë°°ì¹˜ ì €ì¥"""
    if df.empty:
        return
        
    sql = """
        INSERT INTO job_raw (
            platform, title, company, description, location, experience,
            employment_type, posted_date, deadline_date, crawled_at, link, rec_idx
        ) 
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) AS new
        ON DUPLICATE KEY UPDATE crawled_at = new.crawled_at
    """
    df = df.where(pd.notnull(df), None)
    df = df[['platform', 'title', 'company', 'description', 'location', 
             'experience', 'employment_type', 'posted_date', 'deadline_date',
             'crawled_at', 'link', 'rec_idx']]
    data = df.to_records(index=False).tolist()

    async with pool.acquire() as conn:
        async with conn.cursor() as cur:
            try:
                await cur.executemany(sql, data)
                await conn.commit()
                print(f"âœ… {len(df)}ê°œì˜ ë°ì´í„°ë¥¼ job_rawì— ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"ğŸš¨ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                await conn.rollback()

async def get_existing_rec_idx(pool):
    """DBì—ì„œ ê¸°ì¡´ rec_idx ì¡°íšŒ"""
    existing = set()
    async with pool.acquire() as conn:
        async with conn.cursor() as cur:
            await cur.execute("SELECT rec_idx FROM job_raw WHERE platform = 0") # ì‚¬ëŒì¸(0) ê³µê³ ë§Œ
            rows = await cur.fetchall()
            for row in rows:
                if row[0]: existing.add(str(row[0]))
    print(f"âœ… DBì—ì„œ {len(existing)}ê°œì˜ ê¸°ì¡´ ì‚¬ëŒì¸ rec_idxë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    return existing

async def delete_expired_jobs(pool):
    """ë§ˆê°ì¼ì´ ì§€ë‚œ ì±„ìš© ê³µê³ ì™€ ë“±ë¡ì¼ì´ 30ì¼ ì´ìƒ ì§€ë‚œ ì±„ìš© ê³µê³ ë¥¼ DBì—ì„œ ì‚­ì œ"""
    today_date = datetime.now()
    cutoff_date = (today_date - timedelta(days=30)).strftime('%Y-%m-%d')
    today_str_for_sql = today_date.strftime('%Y-%m-%d')

    sql = """
        DELETE FROM job_raw
        WHERE 
            STR_TO_DATE(posted_date, '%%Y-%%m-%%d') < %s
            OR 
            (
                deadline_date REGEXP '^[0-9]{4}/[0-9]{2}/[0-9]{2}$' AND
                STR_TO_DATE(REPLACE(deadline_date, '/', '-'), '%%Y-%%m-%%d') < %s
            )
    """
    async with pool.acquire() as conn:
        async with conn.cursor() as cur:
            await cur.execute(sql, (cutoff_date, today_str_for_sql))
            deleted_count = cur.rowcount
            await conn.commit()
    print(f"âœ… {deleted_count}ê°œì˜ ë§Œë£Œëœ ì±„ìš© ê³µê³ ë¥¼ ì‚­ì œ ì™„ë£Œ")
    
# --- MAIN EXECUTION ---
async def main():
    start_time = datetime.now()
    pool = await aiomysql.create_pool(
        host=os.getenv("DB_HOST"), port=int(os.getenv("DB_PORT")),
        user=os.getenv("DB_USER"), password=os.getenv("DB_PASSWORD"),
        db=os.getenv("DB_NAME", "jobdb"), autocommit=False
    )

    sql_cols = {
        'í”Œë«í¼': 'platform', 'ì œëª©': 'title', 'íšŒì‚¬ëª…': 'company', 'ìƒì„¸ë‚´ìš©': 'description',
        'ì§€ì—­': 'location', 'ê²½ë ¥': 'experience', 'ê³ ìš©í˜•íƒœ': 'employment_type',
        'ë“±ë¡ì¼': 'posted_date', 'ë§ˆê°ì¼': 'deadline_date',
        'í¬ë¡¤ë§ ì‹œê°„': 'crawled_at', 'ìƒì„¸ë§í¬': 'link', 'rec_idx': 'rec_idx'
    }

    await delete_expired_jobs(pool)
    existing_ids = await get_existing_rec_idx(pool)

    all_jobs_buffer = []
    total_new_jobs_count = 0
    
    search_start_date = datetime.strptime(SEARCH_START_DATE_STR, "%Y-%m-%d")
    print(f"--- {search_start_date.strftime('%Y-%m-%d')} ì´í›„ ì±„ìš© ê³µê³  ìˆ˜ì§‘ ì‹œì‘ ---")

    for keyword in SEARCH_KEYWORDS:
        print(f"\n--- í‚¤ì›Œë“œ: '{keyword}' í¬ë¡¤ë§ ì‹œì‘ ---")
        
        async for jobs_from_page in crawl_saramin(search_start_date, existing_ids, TOTAL_PAGE_LIMIT, keyword):
            all_jobs_buffer.extend(jobs_from_page)
            
            if len(all_jobs_buffer) >= BATCH_SIZE:
                batch_to_save = all_jobs_buffer[:BATCH_SIZE]
                all_jobs_buffer = all_jobs_buffer[BATCH_SIZE:]

                df = pd.DataFrame(batch_to_save)
                df.rename(columns=sql_cols, inplace=True)
                df['platform'] = 0 # ì‚¬ëŒì¸ í”Œë«í¼ ë²ˆí˜¸
                await save_final_jobs(pool, df)
                total_new_jobs_count += len(batch_to_save)

    if all_jobs_buffer:
        df = pd.DataFrame(all_jobs_buffer)
        df.rename(columns=sql_cols, inplace=True)
        df['platform'] = 0
        await save_final_jobs(pool, df)
        total_new_jobs_count += len(all_jobs_buffer)

    print(f"\nì´ {total_new_jobs_count}ê°œì˜ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ë° ì €ì¥ ì™„ë£Œ.")
    
    pool.close()
    await pool.wait_closed()
    print(f"ì´ ì†Œìš” ì‹œê°„: {datetime.now() - start_time}")

if __name__ == "__main__":
    asyncio.run(main())